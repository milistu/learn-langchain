{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using `load_dotenv(find_dotenv())` to find and load `.env` file. <br>\n",
    "Why? ðŸ¤” <br> \n",
    "If your environemnt file is not located in the root of your project we can still access it. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "LLMs [documentation](https://python.langchain.com/docs/modules/model_io/llms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"], organization=os.environ[\"OPENAI_ORGANIZATION\"]\n",
    ")\n",
    "chat_model = ChatOpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    organization=os.environ[\"OPENAI_ORGANIZATION\"],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM objects take string as input and output string. <br>\n",
    "The ChatModel objects take a list of messages as input and output a message. <br>\n",
    "For a deeper conceptual explanation of this difference please see [this documentation](https://python.langchain.com/docs/modules/model_io/concepts). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'LLM response: \\n\\nNo, I am a digital AI created by humans.'\n",
      "\"LLM response type: <class 'str'>\"\n",
      "'----------------------------------------------------------------------------------------------------'\n",
      "(\"Chat Model response: content='No, I am an artificial intelligence created by \"\n",
      " \"humans to assist with answering questions and providing information.' \"\n",
      " \"response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': \"\n",
      " \"12, 'total_tokens': 31}, 'model_name': 'gpt-3.5-turbo', \"\n",
      " \"'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': \"\n",
      " 'None}')\n",
      "\"Chat Model response type: <class 'langchain_core.messages.ai.AIMessage'>\"\n"
     ]
    }
   ],
   "source": [
    "text = \"Are you an Alien?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "response = llm.invoke(text)\n",
    "pprint(f\"LLM response: {response}\")\n",
    "pprint(f\"LLM response type: {type(response)}\")\n",
    "\n",
    "pprint(\"-\" * 100)\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "pprint(f\"Chat Model response: {response}\")\n",
    "pprint(f\"Chat Model response type: {type(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with raw LLMs is that they donâ€™t remember the history of the conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Your previous question was \"What is your favorite color?\"', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})\n"
     ]
    }
   ],
   "source": [
    "response = chat_model.invoke(\"What was my previous question?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "Chains [documentation](https://python.langchain.com/docs/modules/chains)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add memory manually \n",
    "Following their current documentation and supported modules we can create a memory modeul and add history manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see chain's process of toughts we neeed to activate **debugging** mode and **verbose**.\n",
    "> ðŸ“Ž **Note**: For some reason **verbose** is not working so we can use debug mode. Debug mode outputs a lot of text that is not important to us now so we will continu using basic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_debug(value=False)\n",
    "set_verbose(value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='No, I am not an alien. I am a computer program designed to assist and provide information to users. How can I help you today?', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 22, 'total_tokens': 51}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"Are you an Alien?\"}\n",
    "\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Are you an Alien?'),\n",
       "  AIMessage(content='No, I am not an alien. I am a computer program designed to assist and provide information to users. How can I help you today?')]}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your previous question was \"Are you an Alien?\" Is there anything else you would like to ask or discuss?', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 65, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What was my previous question?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add memory automatically\n",
    "We can use chain class that support automatic history tracking but they may be unsupported in the future.\n",
    "\n",
    "Here we can set `verbose` in chain directly and it will work.\n",
    "\n",
    "IMO this approach is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    organization=os.environ[\"OPENAI_ORGANIZATION\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_model, verbose=True, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Are you an Alien?!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No, I am not an alien. I am an artificial intelligence created by humans to assist with various tasks and provide information. I do not have a physical form like aliens are often depicted as having.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Are you an Alien?!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the prompt, we see now that there is a bit more then our simple prompt. <br>\n",
    "We have a system prompt, the history of the conversation, and the humanâ€™s question. <br>\n",
    "The system prompt allows us to give the LLM more context on what needs to be done. <br>\n",
    "Letâ€™s see if it remembers: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Are you an Alien?!\n",
      "AI: No, I am not an alien. I am an artificial intelligence created by humans to assist with various tasks and provide information. I do not have a physical form like aliens are often depicted as having.\n",
      "Human: What was my previous question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your previous question was \"Are you an Alien?!\"'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was my previous question?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
